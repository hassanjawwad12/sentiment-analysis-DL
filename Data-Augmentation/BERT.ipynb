{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eSgu4oUbAGD",
        "outputId": "29117708-0403-4bba-91a7-6ef8efb775df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGG92JZwbMZE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def preprocess_data(df):\n",
        "\n",
        "  # Text Preprocessing\n",
        "  # (Customize these steps as needed)\n",
        "  df['Review Text'] = df['Review Text'].str.lower()  # Convert to lowercase\n",
        "  df['Review Text'] = df['Review Text'].str.replace(r'[^\\w\\s]', '', regex=True)  # Remove punctuation\n",
        "  df['Review Text'] = df['Review Text'].str.replace(r'\\s+', ' ', regex=True)  # Remove extra spaces\n",
        "\n",
        "  columns_to_retain=['Review Text', 'Sentiment']\n",
        "  df = df.drop(columns=df.columns.difference(columns_to_retain), axis=1)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrYXX5H_bOQv"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('//content/undersampled.csv')\n",
        "\n",
        "processed_data1 = preprocess_data(df.copy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jL5dGD8Qgn2A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Assuming lemmatizer is already defined\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_dataframe(df):\n",
        "    # Handle missing values\n",
        "    df['Review Text'].fillna(\"\", inplace=True)\n",
        "\n",
        "    # Apply lemmatization\n",
        "    df['Review Text'] = df['Review Text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
        "\n",
        "    # Select only the required columns\n",
        "    df = df[['Review Text', 'Sentiment']]\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUIgZ-yObMDO"
      },
      "outputs": [],
      "source": [
        "processed_data1=preprocess_dataframe(processed_data1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QqUo15-aBVv",
        "outputId": "7934435d-7831-40eb-9695-a80305f3286d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Epoch 1:  33%|███▎      | 87/264 [59:08<1:58:02, 40.02s/it]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load dataset into pandas DataFrame (replace this with your own dataset loading)\n",
        "# Assuming the dataset has 'text' and 'label' columns\n",
        "\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "train_df, test_df = train_test_split(processed_data1, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize input data\n",
        "def tokenize_data(data, max_length):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in data[\"Review Text\"]:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                            text,\n",
        "                            add_special_tokens=True,\n",
        "                            max_length=max_length,\n",
        "                            padding='max_length',\n",
        "                            truncation=True,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt'\n",
        "                       )\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    labels = torch.tensor(data[\"Sentiment\"].tolist())\n",
        "\n",
        "    return input_ids, attention_masks, labels\n",
        "\n",
        "# Tokenize training and testing data\n",
        "max_length = 128\n",
        "train_input_ids, train_attention_masks, train_labels = tokenize_data(train_df, max_length)\n",
        "test_input_ids, test_attention_masks, test_labels = tokenize_data(test_df, max_length)\n",
        "\n",
        "# Create DataLoader for training and testing data\n",
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "test_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "# Load pre-trained BERT model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=2, # Assuming binary classification (change it accordingly)\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False\n",
        ")\n",
        "\n",
        "# Set device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Set hyperparameters\n",
        "epochs = 15\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_dataloader, desc=\"Epoch {}\".format(epoch+1)):\n",
        "        batch_input_ids = batch[0].to(device)\n",
        "        batch_attention_masks = batch[1].to(device)\n",
        "        batch_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=batch_input_ids,\n",
        "            attention_mask=batch_attention_masks,\n",
        "            labels=batch_labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(\"Average training loss: {:.4f}\".format(avg_train_loss))\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "total_accuracy = 0\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
        "        batch_input_ids = batch[0].to(device)\n",
        "        batch_attention_masks = batch[1].to(device)\n",
        "        batch_labels = batch[2].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=batch_input_ids,\n",
        "            attention_mask=batch_attention_masks,\n",
        "            labels=batch_labels\n",
        "        )\n",
        "\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        total_accuracy += torch.sum(preds == batch_labels).item()\n",
        "\n",
        "accuracy = total_accuracy / len(test_df)\n",
        "print(\"Accuracy on test set: {:.2f}%\".format(accuracy * 100))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}